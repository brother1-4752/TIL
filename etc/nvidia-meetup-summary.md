# NVIDIA ì„¸ë¯¸ë‚˜ ë°œí‘œìë£Œ

# LLMì´ë€?

- **í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ í›ˆë ¨ëœ ì¼ì¢…ì˜ AIëª¨ë¸**
- **ëª©í‘œ : ì¸ê°„ê³¼ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ìƒì„±**
    - **Ex. ChatGPT**

<aside>
ğŸ’¡ **ChatGPTëŠ” LLMìœ¼ë¡œì„œ, ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì¸ê°„ê³¼ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ êµìœ¡ì„ ë°›ì•˜ìœ¼ë©° ê´‘ë²”ìœ„í•œ ì£¼ì œì™€ ì§ˆë¬¸ì„ ì´í•´í•˜ê³  ì‘ë‹µí•  ìˆ˜ ìˆë‹¤.**

</aside>

# LLM í•™ìŠµ : NeMoì™€ Megatron

## 1-1. NeMo

- **AIëª¨ë¸ ê°œë°œì„ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ Toolkit**
- **Collection of pre-built neural network**
- **Easy to use with High level API**

## 1-2. NeMoì™€ ë¹„ìŠ·í•œ Toolkit

- **TensorFlow**
- **Hugging Facesâ€™s Transformers**
- **AllenNLP**

## 1-3. ê° Toolkitì˜ ì¥ë‹¨ì 

**TensorFlow:**

- **Strengths: Widely-used, large community, pre-trained models, support for distributed training, wide range of tools**
- **Weaknesses: it can require more code and a deeper understanding of the underlying mechanics to use it effectively., Complex API, `less efficient for certain types of models`**

**Hugging Face's Transformers:**

- **Strengths: Specifically built for NLP, designed to be more user-friendly, with simpler APIs and pre-built modules and models. pre-trained models, support for different languages and models architectures**
- **Weaknesses: `Limited to NLP`, relatively new library**

**AllenNLP:**

- **Strengths: Built on Pytorch, focused on NLP, easy to use, pre-built models and modules for various NLP tasks**
- **Weaknesses: `Limited to NLP`, fewer pre-built models and modules**

**NeMo:**

- **Strengths: Modular design, pre-built neural modules, support for distributed training, support for speech and language understanding tasks, designed to be more user-friendly, with simpler APIs and pre-built modules and models.**
- **Weaknesses: Relatively new, `limited to Pytorch`, may not be as well-suited for large-scale distributed training and deployment as Tensorflow,  í…ì„œí”Œë¡œìš°ë§Œí¼ `ì‚¬ì „ êµ¬ì¶•ëœ ëª¨ë¸ê³¼ ëª¨ë“ˆì´ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì¼ë¶€ ì‚¬ìš©ìì—ê²Œ ì ‘ê·¼ì„±ì„ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ìˆë‹¤.`**

## 2-1. Megatronì´ë€?

- **ë©”ê°€íŠ¸ë¡ (Megatron)ì€ NeMo ìœ„ì— ì„¸ì›Œì§„ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ êµìœ¡í•˜ê¸° ìœ„í•œ `ë¼ì´ë¸ŒëŸ¬ë¦¬`ì´ë‹¤. ì—¬ëŸ¬ GPUì™€ ì—¬ëŸ¬ ê¸°ê³„ì— ê±¸ì³ ì–¸ì–´ ëª¨ë¸ì˜ í›ˆë ¨ì„ í™•ì¥í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆ˜ì‹­ ì–µ ê°œì˜ ë§¤ê°œ ë³€ìˆ˜ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤. `ë©”ê°€íŠ¸ë¡ ì˜ ì£¼ìš” ì´ˆì ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í™•ì¥ì„±ê³¼ ì„±ëŠ¥`ì— ìˆë‹¤.**
- **ë©”ê°€íŠ¸ë¡ ì€ `ëª¨ë¸ ë³‘ë ¬í™”`ë¼ëŠ” ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í˜• ëª¨ë¸ì„ ì—¬ëŸ¬ GPUì— ê±¸ì³ ë¶„í• í•˜ê³  ì´ë“¤ ê°„ì˜ í†µì‹ ì„ ì²˜ë¦¬í•œë‹¤. ì´ë¥¼ í†µí•´ ìˆ˜ì‹­ ì–µ ê°œì˜ ë§¤ê°œ ë³€ìˆ˜ë¡œ ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ì‚¬ìš©ìê°€ ìµœì†Œí•œì˜ ì½”ë“œ ë³€ê²½ìœ¼ë¡œ ëª¨ë¸ì„ êµìœ¡í•  ìˆ˜ ìˆëŠ” PyTorchì™€ ê°™ì€ ì¸ê¸° ìˆëŠ” ë”¥ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì™€ í˜¸í™˜ë˜ëŠ” ê°„ë‹¨í•˜ê³  ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ APIë¥¼ ì œê³µí•œë‹¤.**
- **ì´ ì™¸ì—ë„ ë©”ê°€íŠ¸ë¡ ì€ ì •ë°€ë„ê°€ ë‚®ì€ ë°ì´í„° ìœ í˜•ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ê³¼ì •ì˜ ì†ë„ë¥¼ ë†’ì´ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ìµœì í™” ê¸°ë²•ì¸ `í˜¼í•© ì •ë°€ í›ˆë ¨`ì„ ì§€ì›í•œë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ ì¤„ì´ê³  êµìœ¡ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°€ì†í™”í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤.**
- **ë˜í•œ ë©”ê°€íŠ¸ë¡ ì€ ì—¬ëŸ¬ ê¸°ê³„ì— ê±¸ì³ ëª¨ë¸ì˜ í›ˆë ¨ì„ í™•ì¥í•  ìˆ˜ ìˆëŠ” `ë¶„ì‚° í›ˆë ¨`ë„ ì§€ì›í•œë‹¤. ì´ ê¸°ëŠ¥ì€ ë§¤ìš° í° ëª¨ë¸ë¡œ ì‘ì—…í•˜ê±°ë‚˜ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¡œ ì‘ì—…í•  ë•Œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤.**
- **ìš”ì•½í•˜ìë©´, ë©”ê°€íŠ¸ë¡ ì€ ì—¬ëŸ¬ GPUì™€ ì—¬ëŸ¬ ê¸°ê³„ì—ì„œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ íš¨ê³¼ì ì´ê³  íš¨ìœ¨ì ìœ¼ë¡œ êµìœ¡í•  ìˆ˜ ìˆëŠ” NeMo ìœ„ì— êµ¬ì¶•ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. ëª¨ë¸ ë³‘ë ¬í™”ì™€ í˜¼í•© ì •ë°€ í›ˆë ¨ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ê³¼ì •ì˜ ì†ë„ë¥¼ ë†’ì´ê³  ë¶„ì‚° í›ˆë ¨ì— ëŒ€í•œ ì§€ì›ë„ ì œê³µí•œë‹¤.**

## 2-2. ëª¨ë¸ ë³‘ë ¬í™” ê°œë…

- **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ êµìœ¡í•  ë•Œ ëª¨ë¸ì˜ ë§¤ê°œ ë³€ìˆ˜ê°€ ë‹¨ì¼ GPUì˜ ë©”ëª¨ë¦¬ì— ë§ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©°, ì´ ê²½ìš° ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ êµìœ¡í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ GPUì— ê±¸ì³ ëª¨ë¸ì„ ë¶„í• í•´ì•¼ í•œë‹¤. ì´ ê¸°ìˆ ì„ `ëª¨ë¸ ë³‘ë ¬í™”`ë¼ê³  í•œë‹¤.**
- **ëª¨ë¸ ë³‘ë ¬í™”ëŠ” ì—¬ëŸ¬ GPUì— ê±¸ì³ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì˜ í›ˆë ¨ì„ í™•ì¥í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë˜ ë‹¤ë¥¸ ê¸°ìˆ ì¸ `ë°ì´í„° ë³‘ë ¬í™”ì™€ ë‹¤ë¥´ë‹¤ëŠ” ì `ì— ì£¼ëª©í•  í•„ìš”ê°€ ìˆë‹¤. ë°ì´í„° ë³‘ë ¬í™”ì—ì„œëŠ” ë™ì¼í•œ ëª¨ë¸ì´ ì—¬ëŸ¬ GPUì— ë³µì œë˜ë©° ê° GPUëŠ” `ë°ì´í„°ì˜` ë‹¤ë¥¸ ë¶€ë¶„ ì§‘í•©ì„ ì²˜ë¦¬í•œë‹¤.**

## 2-3. ê´€ë ¨ ìš©ì–´

- **ì—­ì „íŒŒ**
    - **When training a model using backpropagation and model parallelism, each GPU calculates the gradients for the subset of model parameters stored on that GPU. These gradients are then passed to other GPUs, where they are used to update the corresponding subset of model parameters.**
    - **This means that during the training process, each GPU is responsible for computing gradients for the subset of model parameters that it holds, and then these gradients are communicated between GPUs `to update the model parameters.` By splitting the model across multiple GPUs, it allows for larger models to be trained and also allows for faster training as the computation can be done in parallel across GPUs. However, this approach requires more complex communication and coordination between GPUs, which can add some overhead to the training process.**
- **ê·¸ë ˆë””ì–¸íŠ¸**
    - **In the context of machine learning, a gradient is a vector of partial derivatives of a function. In the case of neural networks, the function is typically the loss function, and the partial derivatives are taken with respect to the model parameters. `The gradient represents the rate of change of the loss function with respect to the model parameters`, and it is used to update the model parameters during training.**
    - **More specifically, `in deep learning, gradients are used in the backpropagation process, which is used to train neural networks.` During the forward pass of the neural network, the input is processed through the layers of the network, and the output is compared to the desired output (or target) to calculate the loss. During the backward pass, the gradient of the loss with respect to the model parameters is calculated, and this gradient is used to update the model parameters using an optimization algorithm such as stochastic gradient descent ([SGD)](https://www.notion.so/NVIDIA-41d05f5114f346948d373275efd0d089). The goal is to minimize the loss function `by updating the parameters in the direction opposite to the gradient.`**
        - **SGD(Stochastic Gradient Descent)**
            - **êµ¬í˜„ ì½”ë“œ**
                
                ```python
                Copy code
                import torch
                
                # Define the model and the loss function
                model = ...
                loss_fn = ...
                
                # Define the optimizer
                optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
                
                # Train the model
                for inputs, labels in training_data:
                    # Forward pass
                    output = model(inputs)
                    loss = loss_fn(output, labels)
                
                    # Zero gradients
                    optimizer.zero_grad()
                
                    # Backward pass
                    loss.backward()
                
                    # Update parameters
                    optimizer.step()
                
                ```
                
                - **In this example, `model` is the neural network, `loss_fn` is the loss function used to calculate the error, and `optimizer` is an instance of the `torch.optim.SGD` class. The `lr` argument passed to the constructor of the `SGD` class is the learning rate, which determines the step size of the updates to the model's parameters.**
                - **The training loop iterates over the training data, and for each iteration, it performs a forward pass through the model to generate an output and calculate the loss. Then it calls `optimizer.zero_grad()` to clear the gradients, `loss.backward()` to calculate the gradients and finally `optimizer.step()` to update the model's parameters using the gradients and the learning rate.**
                - **It's worth noting that this is just a simple example of how SGD can be implemented. In practice, there are many variations and modifications that can be made to the algorithm to improve its performance such as, using different learning rates, implementing momentum, nesterov momentum, or using learning rate schedules.**

# FasterTransformer and Triton

- **FastTransformer**ëŠ” ì–¸ì–´ ë²ˆì—­, ì–¸ì–´ ì´í•´ ë° í…ìŠ¤íŠ¸ ìƒì„±ê³¼ ê°™ì€ íš¨ìœ¨ì ì´ê³  ì •í™•í•œ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì„ ìœ„í•´ ì„¤ê³„ëœ `**ì‹ ê²½ë§ ì•„í‚¤í…ì²˜**`ì…ë‹ˆë‹¤. ì´ê²ƒì€ NLPì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì˜ extenstionì´ë‹¤. Fast TransformerëŠ” ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ í”¼ë“œ í¬ì›Œë“œ ë ˆì´ì–´ë¥¼ ì¬ì„¤ê³„í•˜ê³  ë³´ë‹¤ íš¨ìœ¨ì ì¸ ë°ì´í„° ë ˆì´ì•„ì›ƒì„ ì‚¬ìš©í•˜ì—¬ Transformerì˜ ê³„ì‚° ë³µì¡ì„±ì„ ì¤„ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë” ë¹ ë¥¸ í›ˆë ¨ê³¼ ì¶”ë¡  ì‹œê°„ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
- **Triton**ì€ NVIDIAê°€ ê°œë°œí•œ ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ ë°°í¬í•˜ê¸° ìœ„í•œ `**ì„œë²„ ì¸¡ í”Œë«í¼**`ì´ë‹¤. ë†’ì€ ì„±ëŠ¥, ë‚®ì€ ì§€ì—° ì‹œê°„ ë° ë†’ì€ ì²˜ë¦¬ëŸ‰ìœ¼ë¡œ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì„ ë°°í¬í•˜ê³  ì„œë¹„ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Tritonì€ TensorFlow, PyTorch ë° ONNXë¥¼ í¬í•¨í•œ ê´‘ë²”ìœ„í•œ ëª¨ë¸ ë° í”„ë ˆì„ì›Œí¬ë¥¼ ì§€ì›í•˜ë©°, ì˜¨í”„ë ˆë¯¸ìŠ¤ ì„œë²„, ì—ì§€ ì¥ì¹˜ ë° í´ë¼ìš°ë“œ ê¸°ë°˜ ì¸í”„ë¼ì™€ ê°™ì€ ë‹¤ì–‘í•œ í”Œë«í¼ì— ëª¨ë¸ì„ ë°°í¬í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. íŠ¸ë¦¬í†¤ì€ ë˜í•œ GPUì™€ ê°™ì€ ìì›ì˜ ì¦‰ê°ì ì¸ ìŠ¤ì¼€ì¼ë§ì„ ìˆ˜í–‰í•˜ì—¬ ë‹¤ì–‘í•œ ë¶€í•˜ì™€ íŠ¸ë˜í”½ì„ ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.